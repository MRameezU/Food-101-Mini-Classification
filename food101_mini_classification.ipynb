{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0 - Setup","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade torchvision\n!pip install --upgrade torch\nimport torch\nimport torchvision\nprint(f\"torch version:{torch.__version__}\")\nprint(f\"torchvision version:{torchvision.__version__}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom torch import nn\nfrom torchvision import transforms\n\ntry:\n  from torchinfo import summary\nexcept:\n  !pip install torchinfo\n  from torchinfo import summary","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Device Agnostic Code","metadata":{}},{"cell_type":"code","source":"device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1 - Getting Data\nThe dataset we're going to use for deploying a food101_mini_classification model is...\n\nPizza, steak, sushi 20% dataset (pizza, steak, sushi classes from Food101, random 20% of samples)","metadata":{}},{"cell_type":"code","source":"# Download pizza, steak, sushi images from GitHub\ndata_20_percent_path=download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n                                     destination=\"pizza_steak_sushi_20_percent\")\ndata_20_percent_path","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setup training and test paths\ntrain_dir=data_20_percent_path/\"train\"\ntest_dir=data_20_percent_path/\"test\"\ntrain_dir,test_dir","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 - Creating an EffNetB2 feature extractor\nFeautre extractor = a term for a transfer learning model that has its base layers frozen and output layers (or head layers) customized to a certain problem.\n\nEffNetB2 pretrained model in PyTorch - https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.EfficientNet_B2_Weights","metadata":{}},{"cell_type":"code","source":"import torchvision\n\n# 1. setup pretrained EfficeintB2 weights\neffnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT #DEFAULT means best\n\n# 2. Get EffNetB2 transforms\neffnetb2_transform=effnetb2_weights.transforms()\n\n# 3. Setup pretrained model instance\neffnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)\n\n# 4. Freeze the base layers in the model (this will stop all layers from training)\nfor param in effnetb2.parameters():\n  param.requires_grad=False\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchinfo import summary\n\n# Print EffNetB2 model summary (uncomment for full output)\nsummary(effnetb2,\n        input_size=(1, 3, 224, 224),\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"effnetb2.classifier","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seeds()\neffnetb2.classifier=nn.Sequential(nn.Dropout(p=0.3,inplace=True),\n                                  nn.Linear(in_features=1408,out_features=3))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom torchinfo import summary\n\n# Print EffNetB2 model summary (uncomment for full output)\nsummary(effnetb2,\n        input_size=(1, 3, 224, 224),\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 - Creating a function to make an EffNetB2 feature extractor","metadata":{}},{"cell_type":"code","source":"def create_effnetb2_model(num_classes:int=3,\n                          seed:int=42):\n  weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT\n  transform=weights.transforms()\n  model=torchvision.models.efficientnet_b2(weights=weights)\n\n\n\n  for param in model.parameters():\n    param.requires_grad=False\n  torch.manual_seed(seed)\n  model.classifier=nn.Sequential(nn.Dropout(p=0.3,inplace=True),\n                                 nn.Linear(in_features=1408,out_features=num_classes))\n  return model,transform\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"effnetb2,effnetb2_transforms=create_effnetb2_model(num_classes=3,\n                                                   seed=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchinfo import summary\nsummary(model=effnetb2,\n        input_size=(1,3,288,288),\n        col_names=[\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 - Creating DataLoaders for EffNetB2","metadata":{}},{"cell_type":"code","source":"from going_modular.going_modular import data_setup\ntrain_dataloader_effnetb2,test_dataloader_effnetb2,class_names=data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                    test_dir=test_dir,\n                                                                                    transform=effnetb2_transforms,\n                                                                                    batch_size=32)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataloader_effnetb2),len(test_dataloader_effnetb2),class_names","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 - Training EffNetB2 feature extractor","metadata":{}},{"cell_type":"code","source":"from going_modular.going_modular import engine\n\nloss_fn=torch.nn.CrossEntropyLoss()\n\noptimizer=torch.optim.Adam(params=effnetb2.parameters(),\n                           lr=1e-3)#0.001\n\nset_seeds()\n\neffnetb2_results=engine.train(model=effnetb2,\n                              train_dataloader=train_dataloader_effnetb2,\n                              test_dataloader=test_dataloader_effnetb2,\n                              epochs=10,\n                              optimizer=optimizer,\n                              loss_fn=loss_fn,\n                              device=device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4 - Inspecting EffNetB2 loss curves","metadata":{}},{"cell_type":"code","source":"from helper_functions import plot_loss_curves\n\nplot_loss_curves(effnetb2_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.5 - Saving EffNetB2 feature extractor","metadata":{}},{"cell_type":"code","source":"from going_modular.going_modular import utils\n\n# Save the model\nutils.save_model(model=effnetb2,\n                 target_dir=\"models\",\n                 model_name=\"effnetb2_feature_extractor_food101_mini.pth\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.6 - Inspecting the size of our EffNetB2 feature extractor","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\n# Get the model size in bytes and convert to megabytes\npretrained_effnetb2_model_size = Path(\"models/effnetb2_feature_extractor_food101_mini.pth\").stat().st_size / (1024 * 1024)\nprint(f\"Pretrained EffNetB2 feature extractor model size: {round(pretrained_effnetb2_model_size, 2)} MB\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 - Deployed Gradio app structure\nLet's start to put all of our app files into a single directory:\n```\nColab -> folder with all Gradio files -> upload app files to Hugging Face Spaces -> deploy\n```\nBy the end our file structure will look like this:\n\n```\ndemos/\n└── food101_mini_classification/\n    ├── effnetb2_feature_extractor_food101_mini.pth\n    ├── app.py\n    ├── examples/\n    │   ├── example_1.jpg\n    │   ├── example_2.jpg\n    │   └── example_3.jpg\n    ├── model.py\n    └── requirements.txt\n```\n","metadata":{}},{"cell_type":"code","source":"import shutil\nfrom pathlib import Path\n\n# Create food101_mini_classification demo path\nfood101_mini_classification_demo_path=Path(\"demos/food101_mini_classification/\")\n\n# Remove files that might exist and create a new directory\nif food101_mini_classification_demo_path.exists():\n  shutil.rmtree(food101_mini_classification_demo_path)\n\n# Create the new directory\nfood101_mini_classification_demo_path.mkdir(parents=True,\n                                  exist_ok=True)\n\n!ls demos/food101_mini_classification/\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1 - Creating a folder of example images to use with our food101_mini_classification demo\n\nWhat we want:\n\n*    3 images in an examples/ directory\n*    Images should be from the test set\n","metadata":{}},{"cell_type":"code","source":"import shutil\nfrom pathlib import Path\n\n# Create an examples directory\nfood101_mini_classification_examples_path = food101_mini_classification_demo_path/\"examples\"\nfood101_mini_classification_examples_path.mkdir(parents=True,exist_ok=True)\n\n# Collect three random test dataset image paths\nfood101_mini_classification_examples=[Path('data/pizza_steak_sushi_20_percent/test/pizza/482858.jpg'),\n                          Path('data/pizza_steak_sushi_20_percent/test/pizza/3770514.jpg'),\n                          Path('data/pizza_steak_sushi_20_percent/test/sushi/46797.jpg')]\n\n# copy the three images to the example directory\n\nfor example in food101_mini_classification_examples:\n  destination=food101_mini_classification_examples_path/example.name # .name returns the file name\n  print(f\"[INFO] Copying {example} to {destination}\")\n  shutil.copy2(src=example,\n               dst=destination)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}