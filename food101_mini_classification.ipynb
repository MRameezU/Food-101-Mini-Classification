{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0 - Setup","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade torchvision\n!pip install --upgrade torch\nimport torch\nimport torchvision\nprint(f\"torch version:{torch.__version__}\")\nprint(f\"torchvision version:{torchvision.__version__}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom torch import nn\nfrom torchvision import transforms\n\ntry:\n  from torchinfo import summary\nexcept:\n  !pip install torchinfo\n  from torchinfo import summary","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Device Agnostic Code","metadata":{}},{"cell_type":"code","source":"device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1 - Getting Data\nThe dataset we're going to use for deploying a food101_mini_classification model is...\n\nPizza, steak, sushi 20% dataset (pizza, steak, sushi classes from Food101, random 20% of samples)","metadata":{}},{"cell_type":"code","source":"# Download pizza, steak, sushi images from GitHub\ndata_20_percent_path=download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n                                     destination=\"pizza_steak_sushi_20_percent\")\ndata_20_percent_path","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setup training and test paths\ntrain_dir=data_20_percent_path/\"train\"\ntest_dir=data_20_percent_path/\"test\"\ntrain_dir,test_dir","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 - Creating an EffNetB2 feature extractor\nFeautre extractor = a term for a transfer learning model that has its base layers frozen and output layers (or head layers) customized to a certain problem.\n\nEffNetB2 pretrained model in PyTorch - https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.EfficientNet_B2_Weights","metadata":{}},{"cell_type":"code","source":"import torchvision\n\n# 1. setup pretrained EfficeintB2 weights\neffnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT #DEFAULT means best\n\n# 2. Get EffNetB2 transforms\neffnetb2_transform=effnetb2_weights.transforms()\n\n# 3. Setup pretrained model instance\neffnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)\n\n# 4. Freeze the base layers in the model (this will stop all layers from training)\nfor param in effnetb2.parameters():\n  param.requires_grad=False\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchinfo import summary\n\n# Print EffNetB2 model summary (uncomment for full output)\nsummary(effnetb2,\n        input_size=(1, 3, 224, 224),\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"effnetb2.classifier","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seeds()\neffnetb2.classifier=nn.Sequential(nn.Dropout(p=0.3,inplace=True),\n                                  nn.Linear(in_features=1408,out_features=3))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom torchinfo import summary\n\n# Print EffNetB2 model summary (uncomment for full output)\nsummary(effnetb2,\n        input_size=(1, 3, 224, 224),\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 - Creating a function to make an EffNetB2 feature extractor","metadata":{}},{"cell_type":"code","source":"def create_effnetb2_model(num_classes:int=3,\n                          seed:int=42):\n  weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT\n  transform=weights.transforms()\n  model=torchvision.models.efficientnet_b2(weights=weights)\n\n\n\n  for param in model.parameters():\n    param.requires_grad=False\n  torch.manual_seed(seed)\n  model.classifier=nn.Sequential(nn.Dropout(p=0.3,inplace=True),\n                                 nn.Linear(in_features=1408,out_features=num_classes))\n  return model,transform\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"effnetb2,effnetb2_transforms=create_effnetb2_model(num_classes=3,\n                                                   seed=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchinfo import summary\nsummary(model=effnetb2,\n        input_size=(1,3,288,288),\n        col_names=[\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 - Creating DataLoaders for EffNetB2","metadata":{}},{"cell_type":"code","source":"from going_modular.going_modular import data_setup\ntrain_dataloader_effnetb2,test_dataloader_effnetb2,class_names=data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                    test_dir=test_dir,\n                                                                                    transform=effnetb2_transforms,\n                                                                                    batch_size=32)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataloader_effnetb2),len(test_dataloader_effnetb2),class_names","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 - Training EffNetB2 feature extractor","metadata":{}},{"cell_type":"code","source":"from going_modular.going_modular import engine\n\nloss_fn=torch.nn.CrossEntropyLoss()\n\noptimizer=torch.optim.Adam(params=effnetb2.parameters(),\n                           lr=1e-3)#0.001\n\nset_seeds()\n\neffnetb2_results=engine.train(model=effnetb2,\n                              train_dataloader=train_dataloader_effnetb2,\n                              test_dataloader=test_dataloader_effnetb2,\n                              epochs=10,\n                              optimizer=optimizer,\n                              loss_fn=loss_fn,\n                              device=device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4 - Inspecting EffNetB2 loss curves","metadata":{}},{"cell_type":"code","source":"from helper_functions import plot_loss_curves\n\nplot_loss_curves(effnetb2_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.5 - Saving EffNetB2 feature extractor","metadata":{}},{"cell_type":"code","source":"from going_modular.going_modular import utils\n\n# Save the model\nutils.save_model(model=effnetb2,\n                 target_dir=\"models\",\n                 model_name=\"effnetb2_feature_extractor_food101_mini.pth\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.6 - Inspecting the size of our EffNetB2 feature extractor","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\n# Get the model size in bytes and convert to megabytes\npretrained_effnetb2_model_size = Path(\"models/effnetb2_feature_extractor_food101_mini.pth\").stat().st_size / (1024 * 1024)\nprint(f\"Pretrained EffNetB2 feature extractor model size: {round(pretrained_effnetb2_model_size, 2)} MB\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 - Deployed Gradio app structure\nLet's start to put all of our app files into a single directory:\n```\nColab -> folder with all Gradio files -> upload app files to Hugging Face Spaces -> deploy\n```\nBy the end our file structure will look like this:\n\n```\ndemos/\n└── food101_mini_classification/\n    ├── effnetb2_feature_extractor_food101_mini.pth\n    ├── app.py\n    ├── examples/\n    │   ├── example_1.jpg\n    │   ├── example_2.jpg\n    │   └── example_3.jpg\n    ├── model.py\n    └── requirements.txt\n```\n","metadata":{}},{"cell_type":"code","source":"import shutil\nfrom pathlib import Path\n\n# Create food101_mini_classification demo path\nfood101_mini_classification_demo_path=Path(\"demos/food101_mini_classification/\")\n\n# Remove files that might exist and create a new directory\nif food101_mini_classification_demo_path.exists():\n  shutil.rmtree(food101_mini_classification_demo_path)\n\n# Create the new directory\nfood101_mini_classification_demo_path.mkdir(parents=True,\n                                  exist_ok=True)\n\n!ls demos/food101_mini_classification/\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1 - Creating a folder of example images to use with our food101_mini_classification demo\n\nWhat we want:\n\n*    3 images in an examples/ directory\n*    Images should be from the test set\n","metadata":{}},{"cell_type":"code","source":"import shutil\nfrom pathlib import Path\n\n# Create an examples directory\nfood101_mini_classification_examples_path = food101_mini_classification_demo_path/\"examples\"\nfood101_mini_classification_examples_path.mkdir(parents=True,exist_ok=True)\n\n# Collect three random test dataset image paths\nfood101_mini_classification_examples=[Path('data/pizza_steak_sushi_20_percent/test/pizza/482858.jpg'),\n                          Path('data/pizza_steak_sushi_20_percent/test/pizza/3770514.jpg'),\n                          Path('data/pizza_steak_sushi_20_percent/test/sushi/46797.jpg')]\n\n# copy the three images to the example directory\n\nfor example in food101_mini_classification_examples:\n  destination=food101_mini_classification_examples_path/example.name # .name returns the file name\n  print(f\"[INFO] Copying {example} to {destination}\")\n  shutil.copy2(src=example,\n               dst=destination)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now verify that we can get a list of lists from our `examples/` directory.","metadata":{}},{"cell_type":"code","source":"import os\n# Get example filepaths in a list of lists\nexample_list = [[\"examples/\" + example] for example in os.listdir(food101_mini_classification_examples_path)]\nexample_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 - Moving our trained EffNetB2 model to our food101_mini_classification demo directory","metadata":{}},{"cell_type":"code","source":"import shutil\n\n# Create a source path for our target model\neffnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n\n# Create a destination path for our target model\neffnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n\n# Try to move the model file\ntry:\n  print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n\n  # Move the movel\n  shutil.move(src=effnetb2_foodvision_mini_model_path,\n              dst=effnetb2_foodvision_mini_model_destination)\n\n  print(f\"[INFO] Model move complete.\")\n# If the model has already been moved, check if it exists\nexcept:\n  print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")\n  print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 - Turning off EffNetB2 model into a Python script (model.py)\n\nWe have a saved `.pth` model `state_dict` and want to load it into a model instance.\n\nLet's move our `create_effnetb2_model()` function to a script so we can reuse it.","metadata":{}},{"cell_type":"code","source":"%%writefile demos/food101_mini_classification/model.py\nimport torch\nimport torchvision\n\nfrom torch import nn\n\ndef create_effnetb2_model(num_classes:int=3,\n                          seed:int=42):\n  # 1, 2, 3 Create EffNetB2 pretrained weights, transforms and model\n  weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT\n  transforms=weights.transforms()\n  model=torchvision.models.efficientnet_b2(weights=weights)\n\n  for param in model.parameters():\n    param.requires_grad=False\n    model.classifier=nn.Sequential(\n        nn.Dropout(p=0.3,inplace=True),\n        nn.Linear(in_features=1408,out_features=num_classes)\n    )\n  return model,transforms\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 - Turning our FoodVision Mini Gradio app into a Python script (`app.py`)","metadata":{}},{"cell_type":"code","source":"%%writefile demos/food101_mini_classification/app.py\n### 1. Imports and class names setup ###\nimport gradio as gr\nimport os\nimport torch\n\nfrom model import create_effnetb2_model\nfrom timeit import default_timer as timer\nfrom typing import Tuple,Dict\n\n# Setup class names\nclass_names=[\"pizza\",\"steak\",\"sushi\"]\n### 2. Model and transforms perparation ###\neffnetb2,effnetb2_transforms=create_effnetb2_model(num_classes=len(class_names))\n\n# Load save weights\neffnetb2.load_state_dict(\n    torch.load(\n        f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n        map_location=torch.device(\"cpu\") # load the model to the CPU\n    )\n)\n\n### 3. Predict function ###\ndef predict(img)->Tuple[Dict,float]:\n  # start a timer\n  start_time = timer()\n\n  # Transform the input image for use with EffNetB2\n  img=effnetb2_transforms(img).unsqueeze(0) # unsqueeze = add batch dimension on 0th index\n\n  # Put model into eval mode, make prediction\n  effnetb2.eval()\n  with torch.inference_mode():\n    # Pass transformed image through the model and turn the prediction logits into probaiblities\n    pred_probs=torch.softmax(effnetb2(img),dim=1)\n\n  # Create a prediction label and prediction probability dictionary\n  pred_labels_and_probs = {}\n\n  # pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n  # comment loop below to un-comment line above\n  for i,class_name in enumerate(class_names):\n    pred_labels_and_probs[class_name]=pred_probs[0][i]\n\n  # Calculate pred time\n  end_time=timer()\n  pred_time=round(end_time-start_time,4)\n\n  # Return pred dict and pred time\n  return pred_labels_and_probs,pred_time\n\n### 4. Gradio app ###\n\n# Create title, description and article\n\ntitle=\"FoodVision Mini\"\ndescription = \"An [EfficientNetB2 feature extractor](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.efficientnet_b2) computer vision model to classify images as pizza, steak or sushi.\"\narticle = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/#74-building-a-gradio-interface).\"\n\n# Create example list\n# Create example list\nexample_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n\n# Create the Gradio demo\ndemo = gr.Interface(fn=predict, # maps inputs to outputs\n                    inputs=gr.Image(type=\"pil\"),\n                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"),\n                             gr.Number(label=\"Prediction time (s)\")],\n                    examples=example_list,\n                    title=title,\n                    description=description,\n                    article=article)\n\n# Launch the demo!\ndemo.launch()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.5 - Creating a requirements file for FoodVision Mini (`requirements.txt`)\n\nThe requirements file will tell our Hugging Face Space what software dependencies our app requires.\n\nThe three main ones are:\n\n*    `torch`\n*    `torchvision`\n*    `gradio`","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}